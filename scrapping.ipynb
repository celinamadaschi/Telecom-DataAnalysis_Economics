{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a20fef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_URL = \"https://econpapers.repec.org\"\n",
    "SEARCH_URL = \"https://econpapers.repec.org/scripts/jelsearch.pf\"\n",
    "# Liste des codes JEL principaux (A à Z) comme mentionné dans le PDF [cite: 113]\n",
    "JEL_CODES = ['E', 'F', 'G', 'H', 'I', 'J'] # Exemple réduit pour le test. Ajoutez les autres (A-Z).\n",
    "START_YEAR = 2015\n",
    "END_YEAR = 2025\n",
    "MAX_PAPERS_PER_JEL = 200  # Limite mentionnée dans le PDF \n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'From': 'researcher@university.edu' # Bonne pratique : indiquez un contact\n",
    "}\n",
    "\n",
    "def get_paper_details(paper_url, primary_jel):\n",
    "    \"\"\"\n",
    "    Extrait les détails d'une page individuelle d'article selon la section 2d du PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(paper_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # 1. Titre [cite: 103]\n",
    "        title_tag = soup.find('h1', class_='colored') # Structure standard EconPapers\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "\n",
    "        # 2. Auteurs & Affiliations [cite: 104, 105]\n",
    "        # Sur EconPapers, les auteurs sont souvent listés juste après le titre ou dans des balises spécifiques.\n",
    "        # Cette partie nécessite souvent d'analyser le texte brut si la structure n'est pas stricte.\n",
    "        authors = []\n",
    "        affiliations = []\n",
    "        \n",
    "        # Recherche des infos auteurs (structure variable selon le type de papier)\n",
    "        author_divs = soup.find_all('p', class_='small') \n",
    "        author_text = \"\"\n",
    "        for p in author_divs:\n",
    "            if \"Author\" in p.get_text() or \"@\" in p.get_text():\n",
    "                author_text += p.get_text(strip=True) + \" \"\n",
    "        \n",
    "        # Extraction simplifiée (à raffiner selon le HTML exact rencontré)\n",
    "        authors = author_text if author_text else \"Voir lien\"\n",
    "\n",
    "        # 3. Année & Journal/Series [cite: 106, 108]\n",
    "        # On cherche souvent dans le bloc de citation ou métadonnées\n",
    "        text_content = soup.get_text()\n",
    "        year = \"N/A\"\n",
    "        \n",
    "        # Tentative d'extraction de l'année via Regex (cherche 2015-2025)\n",
    "        date_match = re.search(r'(201[5-9]|202[0-5])', text_content)\n",
    "        if date_match:\n",
    "            year = int(date_match.group(0))\n",
    "        \n",
    "        # Vérification du filtre date [cite: 106]\n",
    "        if year != \"N/A\" and (year < START_YEAR or year > END_YEAR):\n",
    "            return \"OUT_OF_DATE\"\n",
    "\n",
    "        # 4. Codes JEL complets [cite: 102]\n",
    "        jel_section = soup.find(string=re.compile(\"JEL-codes:\"))\n",
    "        jel_codes = \"N/A\"\n",
    "        if jel_section:\n",
    "            # Récupère le texte parent ou suivant\n",
    "            jel_codes = jel_section.find_parent().get_text(strip=True).replace(\"JEL-codes:\", \"\")\n",
    "\n",
    "        # 5. Lien de téléchargement [cite: 109]\n",
    "        download_section = soup.find('div', id='download')\n",
    "        download_link = \"N/A\"\n",
    "        if download_section:\n",
    "            link_tag = download_section.find('a', href=True)\n",
    "            if link_tag:\n",
    "                download_link = BASE_URL + link_tag['href']\n",
    "\n",
    "        return {\n",
    "            \"JEL Primary\": primary_jel,   # [cite: 107]\n",
    "            \"Title\": title,               # [cite: 103]\n",
    "            \"Authors\": authors,           # [cite: 104]\n",
    "            \"Affiliations\": \"Voir Full Text\", # Souvent complexe à isoler sans NLP\n",
    "            \"Year\": year,                 # [cite: 106]\n",
    "            \"JEL Codes\": jel_codes,       # [cite: 102]\n",
    "            \"Journal/Series\": \"Working Paper/Journal\", # À parser plus finement\n",
    "            \"URL\": paper_url              # [cite: 109]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du parsing de {paper_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_repec():\n",
    "    all_data = []\n",
    "\n",
    "    print(f\"Début du scraping pour la période {START_YEAR}-{END_YEAR}...\")\n",
    "\n",
    "    for jel in JEL_CODES:\n",
    "        print(f\"--- Traitement du code JEL: {jel} ---\")\n",
    "        \n",
    "        # Paramètres pour la recherche par code JEL\n",
    "        # Note: EconPapers utilise souvent des formulaires, ici on simule une requête GET simple si possible\n",
    "        # ou on itère sur les pages de catégories.\n",
    "        # URL type: https://econpapers.repec.org/scripts/jelsearch.pf?text=E\n",
    "        params = {'text': jel} \n",
    "        \n",
    "        try:\n",
    "            response = requests.get(SEARCH_URL, params=params, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Trouver les liens vers les papiers dans les résultats de recherche\n",
    "            # Les résultats sont souvent dans des balises <li class=\"inlist\"> ou similaires\n",
    "            # Ceci est une approximation de la structure de EconPapers\n",
    "            links = soup.find_all('a', href=True)\n",
    "            \n",
    "            paper_links = []\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                # Filtrer pour ne garder que les liens vers des articles (souvent /paper/ ou /article/)\n",
    "                if '/paper/' in href or '/article/' in href or '/repec/' in href:\n",
    "                    full_link = BASE_URL + href if href.startswith('/') else href\n",
    "                    if full_link not in paper_links:\n",
    "                        paper_links.append(full_link)\n",
    "            \n",
    "            print(f\"Trouvé {len(paper_links)} papiers potentiels pour {jel}. Traitement des {MAX_PAPERS_PER_JEL} premiers...\")\n",
    "\n",
    "            count = 0\n",
    "            for link in paper_links:\n",
    "                if count >= MAX_PAPERS_PER_JEL:\n",
    "                    break\n",
    "                \n",
    "                # Pause pour être poli envers le serveur (Anti-ban)\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "                \n",
    "                paper_data = get_paper_details(link, jel)\n",
    "                \n",
    "                if paper_data == \"OUT_OF_DATE\":\n",
    "                    continue # On ignore ce papier mais on continue\n",
    "                \n",
    "                if paper_data:\n",
    "                    all_data.append(paper_data)\n",
    "                    count += 1\n",
    "                    print(f\"Scrapé ({count}): {paper_data['Title'][:30]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur globale sur le code {jel}: {e}\")\n",
    "\n",
    "    # Création du DataFrame final\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "# --- EXÉCUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    df_results = scrape_repec()\n",
    "    \n",
    "    # Aperçu des données\n",
    "    print(df_results.head())\n",
    "    \n",
    "    # Sauvegarde CSV comme suggéré pour l'analyse future\n",
    "    df_results.to_csv(\"repec_data_raw.csv\", index=False)\n",
    "    print(\"Données sauvegardées dans repec_data_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc926a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c56b43",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m movies = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfindAll\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mdiv\u001b[39m\u001b[33m'\u001b[39m, class_=\u001b[33m'\u001b[39m\u001b[33mlister-item mode-advanced\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(movies))\n",
      "\u001b[31mAttributeError\u001b[39m: 'Response' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "movies = response.findAll('div', class_='lister-item mode-advanced')\n",
    "print(len(movies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
