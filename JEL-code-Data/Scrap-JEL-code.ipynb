{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d82dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- SELENIUM IMPORTS ---\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------------------------------------\n",
    "BASE_URL = \"https://econpapers.repec.org\"\n",
    "JEL_LETTERS = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
    "CSV_FILENAME = \"RePEc_Full_Database.csv\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Research Scraping)\"}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FONCTION : RÉCUPÉRATION DES LIENS PAR PAGE\n",
    "# ---------------------------------------------------------\n",
    "def get_links_from_current_page(driver):\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=re.compile(r\"/paper/|/article/\")):\n",
    "        href = a[\"href\"]\n",
    "        if \"scripts\" in href or \"pers\" in href:\n",
    "            continue\n",
    "        links.append(urljoin(BASE_URL, href))\n",
    "    return list(dict.fromkeys(links))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FONCTION : MÉTADONNÉES\n",
    "# ---------------------------------------------------------\n",
    "def get_paper_details(url, jel_cat):\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        if response.status_code != 200: \n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        title_tag = soup.find(\"h1\", class_=\"colored\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "\n",
    "        year = None\n",
    "        date_b = soup.find(\"b\", string=re.compile(\"Date:\"))\n",
    "        if date_b:\n",
    "            m = re.search(r\"\\d{4}\", date_b.next_sibling or \"\")\n",
    "            if m: \n",
    "                year = int(m.group())\n",
    "\n",
    "        authors = [m[\"content\"].replace(\",\", \"\") for m in soup.find_all(\"meta\", {\"name\": \"citation_author\"})]\n",
    "        journal_meta = soup.find(\"meta\", {\"name\": \"citation_journal_title\"})\n",
    "        journal = journal_meta.get(\"content\", None) if journal_meta else None\n",
    "        \n",
    "        affil = soup.find(\"span\", id=\"contact\")\n",
    "        affiliations = affil.get_text(\" \", strip=True) if affil else None\n",
    "        pub_type = \"Journal Article\" if \"/article/\" in url else \"Working Paper\"\n",
    "\n",
    "        return {\n",
    "            \"JEL Subject\": jel_cat,\n",
    "            \"Title\": title,\n",
    "            \"Author(s)\": \"; \".join(authors) if authors else \"N/A\",\n",
    "            \"Journal\": journal,\n",
    "            \"Year\": year,\n",
    "            \"Type\": pub_type,\n",
    "            \"Affiliations\": affiliations,\n",
    "            \"URL\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur sur {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MAIN : NAVIGATION ET ÉCRITURE DIRECTE\n",
    "# ---------------------------------------------------------\n",
    "# --- CONFIGURATION DE REPRISE ---\n",
    "START_JEL = \"A\"        # La lettre où le scrapping doit commencer\n",
    "START_PAGE = 1     # La page où le scrapping doit recommencer pour la lettre \n",
    "# --------------------------------\n",
    "def main():\n",
    "    if not os.path.exists(CSV_FILENAME):\n",
    "        pd.DataFrame(columns=[\"JEL Subject\",\"Title\",\"Author(s)\",\"Journal\",\"Year\",\"Type\",\"Affiliations\",\"URL\"]).to_csv(CSV_FILENAME, index=False)\n",
    "\n",
    "    #LANCEMENT DU DRIVER AVEC LES OPTIONS\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")              \n",
    "    options.add_argument(\"--disable-gpu\")           \n",
    "    options.add_argument(\"--no-sandbox\")            \n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    # On filtre la liste des lettres pour commencer à START_JEL\n",
    "    for jel in JEL_LETTERS[JEL_LETTERS.index(START_JEL):]:\n",
    "        print(f\"\\n--- DÉBUT CATÉGORIE JEL: {jel} ---\")\n",
    "        search_url = f\"{BASE_URL}/scripts/search.pf?jel={jel}*&ni=10%20years&inpage=1000\"\n",
    "        driver.get(search_url)\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        try:\n",
    "            submit_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@type='SUBMIT']\")))\n",
    "            submit_btn.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            page_num = 1\n",
    "            \n",
    "            # --- LOGIQUE DE SAUT DE PAGES ---\n",
    "            if jel == START_JEL and START_PAGE > 1:\n",
    "                print(f\"Saut rapide vers la page {START_PAGE}...\")\n",
    "                while page_num < START_PAGE:\n",
    "                    try:\n",
    "                        next_link = driver.find_element(By.XPATH, \"//a[img[@class='rightarrow']]\")\n",
    "                        next_link.click()\n",
    "                        page_num += 1\n",
    "                        if page_num % 10 == 0: \n",
    "                            print(f\"Passage de la page {page_num}...\")\n",
    "                            time.sleep(1) \n",
    "                    except:\n",
    "                        break\n",
    "            # --------------------------------\n",
    "\n",
    "            while True:\n",
    "                print(f\"Extraction Page {page_num} pour JEL {jel}...\")\n",
    "                links = get_links_from_current_page(driver)\n",
    "                \n",
    "                current_batch = []\n",
    "                for link in links:\n",
    "                    details = get_paper_details(link, jel)\n",
    "                    if details:\n",
    "                        current_batch.append(details)\n",
    "                \n",
    "                if current_batch:\n",
    "                    pd.DataFrame(current_batch).to_csv(CSV_FILENAME, mode='a', header=False, index=False)\n",
    "                    current_batch = []\n",
    "\n",
    "                try:\n",
    "                    next_link = driver.find_element(By.XPATH, \"//a[img[@class='rightarrow']]\")\n",
    "                    next_link.click()\n",
    "                    page_num += 1\n",
    "                    time.sleep(3)\n",
    "                except:\n",
    "                    print(f\"Plus de pages pour la lettre {jel}.\")\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la navigation pour {jel}: {e}\")\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
